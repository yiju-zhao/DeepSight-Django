# RAG Agent - ReAct æ¶æ„è¯´æ˜

## æ¦‚è¿°

è¿™ä¸ª RAG Agent ä½¿ç”¨ **ReAct (Reasoning + Acting)** æ¨¡å¼ï¼Œé€šè¿‡è¿­ä»£æ¨ç†å’Œæ£€ç´¢æ¥å›ç­”é—®é¢˜ã€‚

## ğŸ”„ è°ƒç”¨æµç¨‹

### 1. å…¥å£ï¼šchat_service.py

```python
# chat_service.py: create_session_chat_stream()
from notebooks.agents.rag_agent.graph import create_rag_agent
from notebooks.agents.rag_agent.config import RAGAgentConfig

# åˆ›å»ºé…ç½®
config = RAGAgentConfig(
    model_name="gpt-5",
    api_key=api_key,
    retrieval_service=retrieval_service,  # æ³¨å…¥æ£€ç´¢æœåŠ¡
    dataset_ids=["dataset_id"],
    max_iterations=5,
)

# åˆ›å»º agent
agent = create_rag_agent(config)

# åˆå§‹åŒ–çŠ¶æ€
initial_state = {
    "question": "ç”¨æˆ·é—®é¢˜",
    "message_history": [],
    "reasoning_steps": [],
    "executed_queries": [],
    "current_retrieved": [],
    "retrieved_chunks": [],
    "iteration": 0,
    "final_answer": "",
}

# æ‰§è¡Œï¼ˆæµå¼ï¼‰
async for event in agent.astream(initial_state):
    # å¤„ç†äº‹ä»¶
    for node_name, node_state in event.items():
        # æ˜¾ç¤ºçŠ¶æ€æ›´æ–°
        ...

# æˆ–æ‰§è¡Œï¼ˆéæµå¼ï¼‰
final_state = await agent.ainvoke(initial_state)
final_answer = final_state["final_answer"]
```

---

### 2. Agent åˆ›å»ºï¼šgraph.py

```python
# graph.py: create_rag_agent()
def create_rag_agent(config: RAGAgentConfig):
    # åˆå§‹åŒ–æ¨¡å‹
    chat_model = init_chat_model(
        model=f"openai:{config.model_name}",
        api_key=config.api_key,
        temperature=config.temperature,
    )

    # å®šä¹‰èŠ‚ç‚¹
    def reasoning_node(state): ...
    def retrieval_node(state): ...
    def evaluation_node(state): ...
    def synthesize_node(state): ...

    # æ„å»ºå›¾
    graph = StateGraph(RAGReActState)
    graph.add_node("reasoning", reasoning_node)
    graph.add_node("retrieval", retrieval_node)
    graph.add_node("evaluation", evaluation_node)
    graph.add_node("synthesize", synthesize_node)

    # æ·»åŠ è¾¹ï¼ˆæ§åˆ¶æµï¼‰
    graph.add_conditional_edges("reasoning", should_retrieve, {...})
    graph.add_edge("retrieval", "evaluation")
    graph.add_conditional_edges("evaluation", should_continue_reasoning, {...})
    graph.add_edge("synthesize", END)

    return graph.compile()
```

---

### 3. ReAct å¾ªç¯æµç¨‹

```
ç”¨æˆ·é—®é¢˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ReAct Loop (æœ€å¤š 5 è½®) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                â”‚
â”‚  ã€reasoning_nodeã€‘                                             â”‚
â”‚    - ä½¿ç”¨ REASON_PROMPT è®© LLM æ€è€ƒ                             â”‚
â”‚    - LLM ç”Ÿæˆæ¨ç†è¿‡ç¨‹å¹¶è¾“å‡ºæŸ¥è¯¢ï¼ˆåŒ…è£¹åœ¨ç‰¹æ®Šæ ‡è®°ä¸­ï¼‰                â”‚
â”‚    - æå–æŸ¥è¯¢ï¼šextract_between(output, BEGIN_SEARCH_QUERY, END) â”‚
â”‚    - æ›´æ–° state.current_queries                                â”‚
â”‚                                                                â”‚
â”‚  â†“                                                             â”‚
â”‚                                                                â”‚
â”‚  ã€should_retrieve æ¡ä»¶åˆ¤æ–­ã€‘                                    â”‚
â”‚    - å¦‚æœæœ‰æŸ¥è¯¢ â†’ è¿›å…¥ retrieval                                 â”‚
â”‚    - å¦‚æœæ— æŸ¥è¯¢ â†’ è·³åˆ° synthesize                                â”‚
â”‚                                                                â”‚
â”‚  â†“                                                             â”‚
â”‚                                                                â”‚
â”‚  ã€retrieval_nodeã€‘ â† è¿™é‡Œè°ƒç”¨ retrieval å·¥å…·                    â”‚
â”‚    - éå†æ‰€æœ‰ current_queries                                   â”‚
â”‚    - å¯¹æ¯ä¸ªæŸ¥è¯¢ï¼š                                                â”‚
â”‚      â‘  æ£€æŸ¥å»é‡ï¼ˆexecuted_queriesï¼‰                              â”‚
â”‚      â‘¡ è°ƒç”¨ config.retrieval_service.retrieve_chunks()          â”‚
â”‚         å‚æ•°ï¼š                                                   â”‚
â”‚           - question: query                                    â”‚
â”‚           - dataset_ids: config.dataset_ids                    â”‚
â”‚           - similarity_threshold: config.similarity_threshold  â”‚
â”‚           - top_k: config.top_k                                â”‚
â”‚      â‘¢ æå– chunks å¹¶è½¬æ¢ä¸º dict æ ¼å¼                            â”‚
â”‚      â‘£ æ·»åŠ åˆ° all_retrieved                                     â”‚
â”‚    - æ›´æ–° state.current_retrieved å’Œ retrieved_chunks           â”‚
â”‚                                                                â”‚
â”‚  â†“                                                             â”‚
â”‚                                                                â”‚
â”‚  ã€evaluation_nodeã€‘                                            â”‚
â”‚    - æ ¼å¼åŒ–æ£€ç´¢ç»“æœï¼šformat_chunks(current_retrieved)            â”‚
â”‚    - æˆªæ–­å†å²ï¼štruncate_reasoning_history(reasoning_steps)      â”‚
â”‚    - ä½¿ç”¨ RELEVANT_EXTRACTION_PROMPT è®© LLM è¯„ä¼°                â”‚
â”‚    - LLM æå–å¼ºç›¸å…³ä¿¡æ¯ï¼Œè¿‡æ»¤æ— å…³å†…å®¹                             â”‚
â”‚    - å°†è¯„ä¼°ç»“æœåŒ…è£¹åœ¨ BEGIN_SEARCH_RESULT æ ‡è®°ä¸­                  â”‚
â”‚    - æ·»åŠ åˆ° message_history å’Œ reasoning_steps                  â”‚
â”‚                                                                â”‚
â”‚  â†“                                                             â”‚
â”‚                                                                â”‚
â”‚  ã€should_continue_reasoning æ¡ä»¶åˆ¤æ–­ã€‘                          â”‚
â”‚    - å¦‚æœè¾¾åˆ° max_iterations â†’ finish                           â”‚
â”‚    - å¦‚æœ LLM è¯´ "sufficient information" â†’ finish             â”‚
â”‚    - å¦åˆ™ â†’ continueï¼ˆå›åˆ° reasoning_nodeï¼‰                      â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“

ã€synthesize_nodeã€‘
  - åˆå¹¶æ‰€æœ‰ reasoning_steps
  - ä½¿ç”¨ SYNTHESIS_PROMPT ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
  - è¿”å› final_answer
  â†“
è¿”å›ç»™ç”¨æˆ·
```

---

## ğŸ”§ Retrieval å·¥å…·è°ƒç”¨è¯¦è§£

### retrieval_node ä¸­çš„è°ƒç”¨

```python
# graph.py: retrieval_node()
async def retrieval_node(state: RAGReActState) -> RAGReActState:
    queries = state["current_queries"]
    all_retrieved = []

    for query in queries:
        # å»é‡æ£€æŸ¥
        if query in state["executed_queries"]:
            continue

        # è°ƒç”¨æ£€ç´¢æœåŠ¡
        result = config.retrieval_service.retrieve_chunks(
            question=query,                                    # æŸ¥è¯¢å­—ç¬¦ä¸²
            dataset_ids=config.dataset_ids,                   # ["dataset_id_1", ...]
            similarity_threshold=config.similarity_threshold, # 0.4
            top_k=config.top_k,                               # 10
        )

        # æå– chunksï¼ˆresult æ˜¯ RetrievalResult å¯¹è±¡ï¼‰
        chunks = result.chunks  # List[ChunkResponse]

        # è½¬æ¢ä¸º dict æ ¼å¼
        for chunk in chunks:
            chunk_dict = {
                "chunk_id": chunk.id,
                "doc_name": chunk.doc_name,
                "content": chunk.content,
                "similarity": chunk.similarity,
            }
            all_retrieved.append(chunk_dict)

    # æ›´æ–°çŠ¶æ€
    return {
        **state,
        "current_retrieved": all_retrieved,
        "retrieved_chunks": state["retrieved_chunks"] + all_retrieved,
        "executed_queries": state["executed_queries"] + queries,
    }
```

---

### RetrievalService API

```python
# retrieval_service.py
class RetrievalService:
    def retrieve_chunks(
        self,
        question: str,              # æŸ¥è¯¢é—®é¢˜
        dataset_ids: list[str],     # æ•°æ®é›† ID åˆ—è¡¨
        similarity_threshold: float = 0.2,  # ç›¸ä¼¼åº¦é˜ˆå€¼
        top_k: int = 6,             # è¿”å›æ•°é‡
    ) -> RetrievalResult:
        """
        è°ƒç”¨ RAGFlow API æ£€ç´¢æ–‡æ¡£å—ã€‚

        Returns:
            RetrievalResult with:
                - chunks: List[ChunkResponse]
                - total_chunks: int
        """
        # è°ƒç”¨ RAGFlow API
        response = self.ragflow_service.retrieve(
            dataset_ids=dataset_ids,
            question=question,
            similarity_threshold=similarity_threshold,
            limit=top_k,
        )

        # è§£æå¹¶è¿”å›ç»“æœ
        return RetrievalResult(
            chunks=[ChunkResponse(...) for chunk in response.chunks],
            total_chunks=len(response.chunks)
        )
```

---

### æ•°æ®æµ

```
config.retrieval_service (æ³¨å…¥)
  â†“
retrieval_node è°ƒç”¨
  â†“
config.retrieval_service.retrieve_chunks(
    question="æ·±åº¦å­¦ä¹ ;åŒ»ç–—å½±åƒ;è¯Šæ–­",
    dataset_ids=["kb_medical"],
    similarity_threshold=0.4,
    top_k=10
)
  â†“
RetrievalResult {
    chunks: [
        ChunkResponse(
            id="chunk_001",
            doc_name="åŒ»ç–—AIç ”ç©¶æŠ¥å‘Š.pdf",
            content="æ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—å½±åƒè¯Šæ–­ä¸­...",
            similarity=0.92
        ),
        ...
    ]
}
  â†“
è½¬æ¢ä¸º dict æ ¼å¼
  â†“
state["current_retrieved"] = [
    {
        "chunk_id": "chunk_001",
        "doc_name": "åŒ»ç–—AIç ”ç©¶æŠ¥å‘Š.pdf",
        "content": "æ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—å½±åƒè¯Šæ–­ä¸­...",
        "similarity": 0.92
    },
    ...
]
  â†“
ä¼ é€’ç»™ evaluation_node è¿›è¡Œ LLM è¯„ä¼°
```

---

## ğŸ“ é…ç½®å‚æ•°

### RAGAgentConfig

```python
@dataclass
class RAGAgentConfig:
    # æ¨¡å‹
    model_name: str = "gpt-5"
    api_key: Optional[str] = None

    # æ¸©åº¦ï¼ˆä¸åŒé˜¶æ®µï¼‰
    temperature: float = 0.7           # Reasoning
    eval_temperature: float = 0.1      # Evaluation
    synthesis_temperature: float = 0.3 # Synthesis

    # ReAct å¾ªç¯
    max_iterations: int = 5

    # æ£€ç´¢å‚æ•°
    retrieval_service: Optional[object] = None
    dataset_ids: list[str] = []
    similarity_threshold: float = 0.4  # ä» 0.2 æå‡åˆ° 0.4
    top_k: int = 10                    # ä» 6 æå‡åˆ° 10

    # å†å²ç®¡ç†
    keep_first_n_steps: int = 1
    keep_last_n_steps: int = 4
```

---

## ğŸ¯ å…³é”®æ”¹è¿›ç‚¹

### 1. æ— éœ€å•ç‹¬çš„ Tool å®šä¹‰
- **æ—§æ¶æ„**ï¼šéœ€è¦å®šä¹‰ LangChain @toolï¼ŒAgent è°ƒç”¨ tool
- **æ–°æ¶æ„**ï¼šç›´æ¥åœ¨ retrieval_node ä¸­è°ƒç”¨ retrieval_service

### 2. Agent è‡ªä¸»ç”ŸæˆæŸ¥è¯¢
- **æ—§æ¶æ„**ï¼šå°æ¨¡å‹ï¼ˆgpt-4.1-miniï¼‰é¢„å¤„ç†æŸ¥è¯¢
- **æ–°æ¶æ„**ï¼šä¸»æ¨¡å‹ï¼ˆgpt-5ï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”ŸæˆæŸ¥è¯¢

### 3. å¼ºç›¸å…³æ€§è¿‡æ»¤
- **æ—§æ¶æ„**ï¼šæ— è¿‡æ»¤ï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰æ£€ç´¢ç»“æœ
- **æ–°æ¶æ„**ï¼ševaluation_node ä½¿ç”¨ LLM è¯„ä¼°å¹¶è¿‡æ»¤

### 4. è¿­ä»£ä¼˜åŒ–
- **æ—§æ¶æ„**ï¼šå•æ¬¡æ£€ç´¢
- **æ–°æ¶æ„**ï¼šæœ€å¤š 5 è½®ï¼Œæ ¹æ®ç»“æœè´¨é‡å†³å®šæ˜¯å¦ç»§ç»­

---

## ğŸ” è°ƒè¯•æŠ€å·§

### æŸ¥çœ‹æ—¥å¿—
```python
# graph.py ä¸­æœ‰è¯¦ç»†æ—¥å¿—
logger.info(f"[retrieval_node] Retrieved {len(chunks)} chunks for query: {query}")
logger.info(f"[reasoning_node] Extracted {len(queries)} queries: {queries}")
```

### æ£€æŸ¥çŠ¶æ€
```python
final_state = await agent.ainvoke(initial_state)

print("æ‰§è¡Œçš„æŸ¥è¯¢:", final_state["executed_queries"])
print("è¿­ä»£æ¬¡æ•°:", final_state["iteration"])
print("æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°:", len(final_state["retrieved_chunks"]))
print("æ¨ç†æ­¥éª¤:")
for i, step in enumerate(final_state["reasoning_steps"], 1):
    print(f"\nStep {i}:")
    print(step[:200] + "...")
```

### æµ‹è¯•å•ä¸ªèŠ‚ç‚¹
```python
# æµ‹è¯• retrieval_node
test_state = {
    "current_queries": ["æ·±åº¦å­¦ä¹ ;åŒ»ç–—"],
    "executed_queries": [],
    "current_retrieved": [],
    "retrieved_chunks": [],
    ...
}

result = await retrieval_node(test_state)
print("æ£€ç´¢ç»“æœ:", result["current_retrieved"])
```

---

## ğŸ“š ç›¸å…³æ–‡ä»¶

- `states.py` - RAGReActState å®šä¹‰
- `prompts.py` - REASON_PROMPTã€RELEVANT_EXTRACTION_PROMPTã€SYNTHESIS_PROMPT
- `graph.py` - ReAct å¾ªç¯èŠ‚ç‚¹å’Œé€»è¾‘
- `config.py` - RAGAgentConfig é…ç½®
- `utils.py` - è¾…åŠ©å‡½æ•°
- `test_react.py` - æµ‹è¯•è„šæœ¬

---

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€è°ƒç”¨
```python
from notebooks.agents.rag_agent import create_rag_agent, RAGAgentConfig
from notebooks.services.retrieval_service import RetrievalService

# åˆå§‹åŒ–
retrieval_service = RetrievalService(ragflow_service)
config = RAGAgentConfig(
    model_name="gpt-5",
    api_key="sk-...",
    retrieval_service=retrieval_service,
    dataset_ids=["medical_kb"],
)

agent = create_rag_agent(config)

# æ‰§è¡Œ
initial_state = {
    "question": "æ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—å½±åƒè¯Šæ–­ä¸­çš„åº”ç”¨æ•ˆæœå¦‚ä½•ï¼Ÿ",
    "message_history": [],
    "reasoning_steps": [],
    "executed_queries": [],
    "current_reasoning": "",
    "current_queries": [],
    "current_retrieved": [],
    "retrieved_chunks": [],
    "iteration": 0,
    "final_answer": "",
    "should_continue": True,
}

final_state = await agent.ainvoke(initial_state)
print(final_state["final_answer"])
```

### æµå¼è°ƒç”¨
```python
async for event in agent.astream(initial_state):
    for node_name, node_state in event.items():
        if node_name == "reasoning":
            print(f"ğŸ¤” æ€è€ƒä¸­ï¼ˆç¬¬ {node_state['iteration']} è½®ï¼‰...")
        elif node_name == "retrieval":
            print(f"ğŸ” æ£€ç´¢: {node_state['current_queries']}")
        elif node_name == "evaluation":
            print("ğŸ“Š åˆ†æç»“æœ...")
        elif node_name == "synthesize":
            print("âœï¸ ç”Ÿæˆç­”æ¡ˆ...")
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### 1. æ£€ç´¢å¤±è´¥
- æ£€æŸ¥ `retrieval_service` æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
- æ£€æŸ¥ `dataset_ids` æ˜¯å¦å­˜åœ¨
- æŸ¥çœ‹æ—¥å¿—ä¸­çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯

### 2. æ— é™å¾ªç¯
- æ£€æŸ¥ `max_iterations` è®¾ç½®
- æŸ¥çœ‹ `should_continue_reasoning` çš„æ¡ä»¶åˆ¤æ–­
- Agent å¯èƒ½æ— æ³•æ‰¾åˆ° "sufficient information" ä¿¡å·

### 3. ç»“æœè´¨é‡å·®
- è°ƒæ•´ `similarity_threshold`ï¼ˆæé«˜è¿‡æ»¤ï¼‰
- å¢åŠ  `top_k`ï¼ˆæ‰©å¤§å€™é€‰é›†ï¼‰
- æ£€æŸ¥ REASON_PROMPT å’Œ RELEVANT_EXTRACTION_PROMPT

---

**å®Œæˆæ—¶é—´ï¼š** 2025-12-11
**æ¶æ„ï¼š** ReAct (Reasoning + Acting)
**æ¨¡å‹ï¼š** GPT-5
